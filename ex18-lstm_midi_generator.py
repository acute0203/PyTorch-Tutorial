import os
import glob
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from music21 import converter, instrument, note, chord, stream
from collections import Counter

# è¨­å®šè£ç½®ï¼ˆå¯æ”¯æ´ Mac M1/M2 çš„ GPU åŠ é€Ÿï¼‰
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
print(f"âš™ï¸ ä½¿ç”¨è£ç½®ï¼š{device}")

# === Step 1: è®€å– MIDI æª”æ¡ˆä¸¦è§£æç‚ºéŸ³ç¬¦åºåˆ— ===
def load_midi_notes(midi_folder):
    notes = []
    r = midi_folder
    for y in range(2004, 2005):  # æ­¤ç¯„ä¾‹åªè™•ç† 2004 å¹´è³‡æ–™å¤¾ï¼Œå¯è‡ªè¡Œèª¿æ•´
        r_midi_folder = f"{r}/{y}/"
        idx = 0
        for file in glob.glob(os.path.join(r_midi_folder, "*.midi")):
            idx += 1
            print(idx, file)
            midi = converter.parse(file)
            parts = instrument.partitionByInstrument(midi)
            notes_to_parse = parts.parts[0].recurse() if parts else midi.flat.notes
            for element in notes_to_parse:
                if isinstance(element, note.Note):
                    notes.append(str(element.pitch))  # è½‰æ›å–®ä¸€éŸ³ç¬¦ç‚ºå­—ä¸² pitch
                elif isinstance(element, chord.Chord):
                    notes.append('.'.join(str(n) for n in element.normalOrder))  # å’Œå¼¦è½‰ç‚ºå­—ä¸²
    return notes

# === Step 2: è½‰æ›éŸ³ç¬¦ç‚ºæ•¸å€¼åºåˆ—ï¼Œæº–å‚™è¨“ç·´è³‡æ–™ ===
def prepare_sequences(notes, sequence_length=30, min_freq=1):
    note_counts = Counter(notes)
    frequent_notes = [note for note, count in note_counts.items() if count >= min_freq]
    note_to_int = {note: idx for idx, note in enumerate(frequent_notes)}
    int_to_note = {idx: note for note, idx in note_to_int.items()}

    network_input, network_output = [], []
    for i in range(len(notes) - sequence_length):
        sequence_in = notes[i:i + sequence_length]
        sequence_out = notes[i + sequence_length]
        if all(n in note_to_int for n in sequence_in) and sequence_out in note_to_int:
            network_input.append([note_to_int[n] for n in sequence_in])
            network_output.append(note_to_int[sequence_out])

    n_patterns = len(network_input)
    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1)) / float(len(note_to_int))
    network_output = np.array(network_output)
    print(f"å¯ç”¨è¨“ç·´æ¨£æœ¬æ•¸é‡ï¼š{len(network_input)}ï¼ŒNoteç¨®é¡ï¼š{len(note_to_int)}")
    return network_input, network_output, note_to_int, int_to_note

# === Step 3: å®šç¾© LSTM æ¨¡å‹æ¶æ§‹ ===
class MusicLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MusicLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=2, batch_first=True)
        self.dropout = nn.Dropout(0.3)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out, _ = self.lstm(x)
        out = self.dropout(out)
        out = self.fc(out[:, -1, :])  # åªå–æœ€å¾Œä¸€å€‹æ™‚é–“é»çš„è¼¸å‡ºåšåˆ†é¡
        return out

# === Step 4: æ¨¡å‹è¨“ç·´æµç¨‹ ===
def train(model, network_input, network_output, epochs=20, batch_size=32, lr=0.001):
    model.to(device)  # æ¨¡å‹ä¸Ÿåˆ° GPU æˆ– CPU
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    for epoch in range(epochs):
        print(f"EPOCH:{epoch}")
        epoch_loss = 0
        count = 0
        for i in range(0, len(network_input) - batch_size, batch_size):
            inputs = torch.tensor(network_input[i:i+batch_size], dtype=torch.float32).to(device)
            targets = torch.tensor(network_output[i:i+batch_size], dtype=torch.long).to(device)

            outputs = model(inputs)
            loss = criterion(outputs, targets)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            count += 1

        if count > 0:
            avg_loss = epoch_loss / count
            print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")
        else:
            print(f"Epoch {epoch+1}/{epochs}, âš ï¸ è·³éï¼šè³‡æ–™ä¸è¶³ä»¥å½¢æˆä¸€å€‹ batch")

# === Step 5: ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹ç”ŸæˆéŸ³æ¨‚ MIDI ===
def generate_music(model, network_input, int_to_note, note_count, output_file='output.mid'):
    if len(network_input) < 1:
        print("âŒ ç„¡æ³•ç”ŸæˆéŸ³æ¨‚ï¼Œnetwork_input å¤ªçŸ­")
        return

    start = np.random.randint(0, len(network_input)-1)
    pattern = network_input[start]
    pattern = torch.tensor(pattern, dtype=torch.float32).unsqueeze(0).to(device)

    generated_notes = []

    model.eval()
    with torch.no_grad():
        for _ in range(500):
            prediction = model(pattern)
            _, index = torch.max(prediction, 1)
            result = int_to_note[index.item()]
            generated_notes.append(result)

            new_input = torch.tensor([[index.item() / float(note_count)]], dtype=torch.float32).to(device)
            pattern = torch.cat((pattern[:, 1:, :], new_input.unsqueeze(0)), dim=1)

    offset = 0
    output_notes = []
    for pattern in generated_notes:
        if ('.' in pattern) or pattern.isdigit():
            notes_in_chord = pattern.split('.')
            notes = [note.Note(int(n)) for n in notes_in_chord]
            for n in notes:
                n.storedInstrument = instrument.Piano()
            new_chord = chord.Chord(notes)
            new_chord.offset = offset
            output_notes.append(new_chord)
        else:
            new_note = note.Note(pattern)
            new_note.offset = offset
            new_note.storedInstrument = instrument.Piano()
            output_notes.append(new_note)
        offset += 0.5

    midi_stream = stream.Stream(output_notes)
    midi_stream.write('midi', fp=output_file)
    print(f"âœ… MIDI æª”å·²å„²å­˜ç‚ºï¼š{output_file}")

# === Step 6: ä¸»ç¨‹å¼æµç¨‹ ===
if __name__ == "__main__":
    midi_dir = "maestro-v3.0.0"

    print("ğŸ“¥ é–‹å§‹è®€å– MIDI...", flush=True)
    notes = load_midi_notes(midi_dir)
    print(f"âœ… è®€å–å®Œæˆï¼Œå…± {len(notes)} å€‹éŸ³ç¬¦", flush=True)

    print("ğŸ” æº–å‚™åºåˆ—è³‡æ–™...", flush=True)
    net_in, net_out, note_to_int, int_to_note = prepare_sequences(notes)
    print(f"âœ… åºåˆ—è³‡æ–™æº–å‚™å®Œæˆï¼Œå…± {len(net_in)} ç­†è¨“ç·´æ¨£æœ¬", flush=True)

    print("ğŸ§  å»ºç«‹ LSTM æ¨¡å‹...", flush=True)
    model = MusicLSTM(input_size=1, hidden_size=256, output_size=len(note_to_int))
    print("âœ… æ¨¡å‹å»ºç«‹å®Œæˆ", flush=True)

    print("ğŸš€ é–‹å§‹è¨“ç·´æ¨¡å‹...", flush=True)
    train(model, net_in, net_out, epochs=30)
    print("âœ… è¨“ç·´å®Œæˆ", flush=True)

    print("ğŸ¼ ç”ŸæˆéŸ³æ¨‚...", flush=True)
    generate_music(model, net_in, int_to_note, note_count=len(note_to_int))
    print("âœ… éŸ³æ¨‚ç”Ÿæˆå®Œæˆ", flush=True)

# === ğŸ“š ç·´ç¿’é¡Œç›® ===
# 1. èª¿æ•´ sequence_lengthï¼ˆä¾‹å¦‚ï¼š20ã€50ï¼‰è§€å¯Ÿè¨“ç·´æ¨£æœ¬æ•¸è®ŠåŒ–
# 2. å˜—è©¦ä¸åŒ dropout å€¼ï¼ˆå¦‚ 0.1ã€0.5ï¼‰è§€å¯Ÿ overfitting ç‹€æ³
# 3. ä¿®æ”¹ hidden_sizeï¼ˆå¦‚ 128 æˆ– 512ï¼‰è§€å¯Ÿç”ŸæˆéŸ³æ¨‚å“è³ª
# 4. å°‡ output æ”¹ç‚º top-k sampling è€Œé maxï¼Œå¢åŠ éŸ³æ¨‚å¤šæ¨£æ€§
# 5. å˜—è©¦åŠ å…¥ BatchNorm æˆ– GRU æ›¿ä»£ LSTMï¼Œè§€å¯Ÿè¨“ç·´æ”¶æ–‚æ€§